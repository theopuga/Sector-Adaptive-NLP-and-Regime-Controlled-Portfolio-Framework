#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Combine all parquet shards into a single filings_clean.parquet file.

This script reads the manifest CSV generated by make_sec_features.py and combines
all 'fused' flavor shards (which have a single 'text' column) into one parquet file.

The output is designed to be used by build_word_trends.py which expects:
- A parquet file with 'text' and 'date' columns
- The date column can be derived from 'filing_date'

Usage:
    python run/combine_shards_to_filings_clean.py \
        --paths config/paths.yaml \
        --manifest_key filings_clean_manifest \
        --output data/textsec/interim/filings_clean.parquet \
        --flavor fused \
        --chunk_size 1000
"""

from __future__ import annotations
import argparse
from pathlib import Path
import pandas as pd
import yaml

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # Simple tqdm replacement
    class tqdm:
        def __init__(self, iterable, **kwargs):
            self.iterable = iterable
            self.total = len(iterable) if hasattr(iterable, '__len__') else None
            self.desc = kwargs.get('desc', '')
        def __iter__(self):
            return iter(self.iterable)
        def set_postfix(self, **kwargs):
            pass

def load_manifest(manifest_path: Path, flavor: str = "fused") -> pd.DataFrame:
    """Load manifest and filter by flavor."""
    man = pd.read_csv(manifest_path)
    if "flavor" in man.columns:
        man = man[man["flavor"] == flavor].copy()
    print(f"üìã Loaded manifest: {len(man)} shards (flavor='{flavor}')")
    return man.reset_index(drop=True)

def validate_shard_columns(df: pd.DataFrame, shard_path: str) -> tuple[bool, list[str]]:
    """Validate that shard has required columns."""
    required = {"text"}
    date_candidates = {"filing_date", "date"}
    
    missing_required = required - set(df.columns)
    if missing_required:
        return False, [f"Missing required: {missing_required}"]
    
    has_date = any(cand in df.columns for cand in date_candidates)
    if not has_date:
        return False, [f"Missing date column (tried: {date_candidates})"]
    
    return True, []

def write_shards_to_parquet(manifest: pd.DataFrame, 
                            base_path: Path,
                            output_path: Path,
                            chunk_size: int = 1000,
                            progress: bool = True) -> dict:
    """
    Stream shards directly to parquet file without loading everything into memory.
    
    Returns dict with statistics: {rows, columns, date_min, date_max, failed_shards}
    """
    import pyarrow as pa
    import pyarrow.parquet as pq
    
    failed_shards = []
    total_rows = 0
    schema_written = False
    writer = None
    date_min = None
    date_max = None
    columns_seen = None
    
    iterator = tqdm(manifest.iterrows(), total=len(manifest), desc="Reading & writing shards") if progress else manifest.iterrows()
    
    try:
        shard_count = 0
        for idx, row in iterator:
            shard_count += 1
            shard_path_str = row["shard_path"]
            
            # Handle both absolute and relative paths
            if Path(shard_path_str).is_absolute():
                shard_path = Path(shard_path_str)
            else:
                shard_path = base_path / shard_path_str if base_path.exists() else Path(shard_path_str)
            
            # Try relative to current working directory if still not found
            if not shard_path.exists():
                shard_path = Path.cwd() / shard_path_str
            
            if not shard_path.exists():
                print(f"‚ö†Ô∏è  Warning: Shard not found: {shard_path_str}")
                failed_shards.append(shard_path_str)
                continue
            
            try:
                df_shard = pd.read_parquet(shard_path)
                
                if df_shard.empty:
                    continue
                
                # Validate columns
                valid, errors = validate_shard_columns(df_shard, str(shard_path))
                if not valid:
                    print(f"‚ö†Ô∏è  Warning: Invalid shard {shard_path.name}: {errors}")
                    failed_shards.append(shard_path_str)
                    continue
                
                # Ensure 'date' column exists (map from filing_date if needed)
                if "date" not in df_shard.columns and "filing_date" in df_shard.columns:
                    df_shard = df_shard.copy()
                    df_shard["date"] = pd.to_datetime(df_shard["filing_date"], errors="coerce")
                
                # Normalize date to month start (as expected by build_word_trends.py)
                if "date" in df_shard.columns:
                    df_shard["date"] = pd.to_datetime(df_shard["date"], errors="coerce")
                    df_shard["date"] = df_shard["date"].dt.to_period("M").dt.to_timestamp()
                
                # Track date range
                if "date" in df_shard.columns and df_shard["date"].notna().any():
                    shard_date_min = df_shard["date"].min()
                    shard_date_max = df_shard["date"].max()
                    if date_min is None or (pd.notna(shard_date_min) and shard_date_min < date_min):
                        date_min = shard_date_min
                    if date_max is None or (pd.notna(shard_date_max) and shard_date_max > date_max):
                        date_max = shard_date_max
                
                # Ensure ID columns are strings (before schema creation)
                for col in ["gvkey", "iid", "cik", "ticker", "isin", "figi"]:
                    if col in df_shard.columns:
                        # Convert to string, handling NaN/None properly
                        df_shard[col] = df_shard[col].astype("string")  # pandas StringDtype handles NaN
                
                # Track columns and align schema
                if columns_seen is None:
                    columns_seen = list(df_shard.columns)
                    # Initialize writer on first chunk
                    table = pa.Table.from_pandas(df_shard)
                    schema = table.schema
                    
                    # Force ID columns to be strings in schema (even if first shard had them as numeric)
                    schema_fields = []
                    for field in schema:
                        if field.name in ["gvkey", "iid", "cik", "ticker", "isin", "figi"]:
                            # Force string type for ID columns
                            schema_fields.append(pa.field(field.name, pa.string(), nullable=True))
                        else:
                            schema_fields.append(field)
                    schema = pa.schema(schema_fields)
                    
                    writer = pq.ParquetWriter(output_path, schema, compression='snappy')
                    schema_written = True
                else:
                    # Align columns to match schema
                    df_aligned = df_shard.copy()  # Already has string types from above
                    
                    for col in columns_seen:
                        if col not in df_aligned.columns:
                            # Add missing column with appropriate type from schema
                            field = schema.field(col)
                            if field.type == pa.string() or field.type == pa.large_string():
                                df_aligned[col] = pd.NA
                            elif field.type in (pa.timestamp('ns'), pa.timestamp('us'), pa.timestamp('ms')):
                                df_aligned[col] = pd.NaT
                            elif field.nullable:
                                df_aligned[col] = None
                            else:
                                # Non-nullable field, use default
                                if field.type == pa.int64():
                                    df_aligned[col] = 0
                                elif field.type == pa.float64():
                                    df_aligned[col] = 0.0
                                else:
                                    df_aligned[col] = None
                    
                    # Reorder columns to match schema
                    df_aligned = df_aligned[[col for col in columns_seen if col in df_aligned.columns]]
                    df_shard = df_aligned
                
                # Write chunk to parquet (schema already established)
                if schema_written:
                    table = pa.Table.from_pandas(df_shard, schema=schema)
                    writer.write_table(table)
                
                total_rows += len(df_shard)
                
                # Update progress
                if progress and shard_count % 10 == 0:
                    iterator.set_postfix({"rows": f"{total_rows:,}", "failed": len(failed_shards)})
            
            except Exception as e:
                print(f"‚ö†Ô∏è  Error reading/writing shard {shard_path.name}: {e}")
                failed_shards.append(shard_path_str)
                continue
        
        if writer is not None:
            writer.close()
        
        if not schema_written:
            raise ValueError("‚ùå No valid shards found to combine!")
        
        return {
            "rows": total_rows,
            "columns": columns_seen,
            "date_min": date_min,
            "date_max": date_max,
            "failed_shards": failed_shards
        }
    
    except Exception as e:
        if writer is not None:
            writer.close()
        # Clean up partial file
        if output_path.exists():
            output_path.unlink()
        raise

def main():
    ap = argparse.ArgumentParser(
        description="Combine parquet shards into a single filings_clean.parquet file"
    )
    ap.add_argument("--paths", default="config/paths.yaml", help="Path to paths.yaml")
    ap.add_argument("--manifest_key", default="filings_clean_manifest",
                    help="Key in paths.yaml for manifest CSV (default: filings_clean_manifest)")
    ap.add_argument("--output", default=None,
                    help="Output parquet path (default: from paths.yaml textsec.filings_clean)")
    ap.add_argument("--flavor", default="fused", choices=["fused", "split"],
                    help="Shard flavor to combine (default: fused, which has 'text' column)")
    ap.add_argument("--chunk_size", type=int, default=1000,
                    help="Chunk size for processing (default: 1000)")
    ap.add_argument("--no_progress", action="store_true",
                    help="Disable progress bars")
    args = ap.parse_args()
    
    # Load paths config
    paths = yaml.safe_load(open(args.paths))
    textsec = paths.get("textsec", {})
    
    # Determine manifest path
    manifest_key = args.manifest_key
    if manifest_key in textsec:
        manifest_fp = Path(textsec[manifest_key])
    else:
        manifest_fp = Path("data/textsec/interim/filings_clean_manifest.csv")
    
    if not manifest_fp.exists():
        raise SystemExit(f"‚ùå Manifest not found: {manifest_fp}")
    
    # Determine output path
    if args.output:
        output_fp = Path(args.output)
    elif "filings_clean" in textsec:
        output_fp = Path(textsec["filings_clean"])
    else:
        output_fp = Path("data/textsec/interim/filings_clean.parquet")
    
    output_fp.parent.mkdir(parents=True, exist_ok=True)
    
    print("=== COMBINE SHARDS TO FILINGS CLEAN ===")
    print(f"Manifest: {manifest_fp}")
    print(f"Flavor:   {args.flavor}")
    print(f"Output:   {output_fp}")
    print()
    
    # Load manifest
    manifest = load_manifest(manifest_fp, flavor=args.flavor)
    
    if manifest.empty:
        raise SystemExit(f"‚ùå No shards found with flavor='{args.flavor}' in manifest")
    
    # Determine base path for resolving relative shard paths
    base_path = manifest_fp.parent
    
    # Stream shards directly to parquet (memory-efficient)
    print(f"üíæ Writing shards to {output_fp} (streaming)...")
    stats = write_shards_to_parquet(
        manifest=manifest,
        base_path=base_path,
        output_path=output_fp,
        chunk_size=args.chunk_size,
        progress=not args.no_progress
    )
    
    # Handle failed shards
    if stats["failed_shards"]:
        print(f"\n‚ö†Ô∏è  Failed to load {len(stats['failed_shards'])} shards")
        if len(stats["failed_shards"]) <= 20:
            for fs in stats["failed_shards"]:
                print(f"    - {fs}")
        else:
            print(f"    (showing first 10 of {len(stats['failed_shards'])})")
            for fs in stats["failed_shards"][:10]:
                print(f"    - {fs}")
    
    # Verify output file
    if not output_fp.exists():
        raise SystemExit("‚ùå Output file was not created!")
    
    # Ensure required columns exist
    if "text" not in stats["columns"]:
        raise SystemExit("‚ùå Combined dataset missing required 'text' column")
    
    if "date" not in stats["columns"]:
        raise SystemExit("‚ùå Combined dataset missing required 'date' column")
    
    # Print summary
    file_size_mb = output_fp.stat().st_size / (1024 * 1024)
    print(f"\n‚úÖ Wrote {output_fp}")
    print(f"   Size: {file_size_mb:.2f} MB")
    print(f"   Rows: {stats['rows']:,}")
    print(f"   Columns: {len(stats['columns'])}")
    print(f"   Date range: {stats['date_min']} ‚Üí {stats['date_max']}")
    
    # Show column summary (read schema only, avoid loading data for large files)
    print("\nüìä Column summary:")
    try:
        import pyarrow.parquet as pq
        
        # Read just metadata without loading data
        parquet_file = pq.ParquetFile(output_fp)
        schema = parquet_file.schema
        
        # Get column info from schema
        schema_dict = {field.name: field for field in schema}
        for col in stats["columns"]:
            if col in schema_dict:
                field = schema_dict[col]
                dtype_str = str(field.type)
                nullable = "nullable" if field.nullable else "required"
                print(f"   {col:20s} {dtype_str:20s} ({nullable})")
            else:
                print(f"   {col:20s} (not in schema)")
        
        # Try to read a very small sample for null counts (only if file is reasonable size)
        if file_size_mb < 1000:  # Only for files < 1GB
            try:
                # Use pyarrow to read just the first row group
                pf = pq.ParquetFile(output_fp)
                if pf.num_row_groups > 0:
                    first_rg = pf.read_row_group(0)
                    sample_df = first_rg.to_pandas()
                    if len(sample_df) > 0 and len(sample_df) <= 10000:  # Sanity check
                        print(f"\n   Sample null counts (from first row group, {len(sample_df)} rows):")
                        for col in stats["columns"]:
                            if col in sample_df.columns:
                                null_pct = (sample_df[col].isna().sum() / len(sample_df)) * 100
                                print(f"   {col:20s} {null_pct:5.1f}% null in sample")
            except Exception:
                pass  # Skip sample reading if it fails
                
    except Exception as e:
        print(f"   (Could not read schema: {e})")
        print(f"   Columns: {', '.join(stats['columns'])}")

if __name__ == "__main__":
    main()
